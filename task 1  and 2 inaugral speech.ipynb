{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thirty-three',\n",
       " 'Scotty',\n",
       " 'did',\n",
       " 'not',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'school',\n",
       " '.',\n",
       " 'His']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "#brown.categories()\n",
    "brown.words(categories='fiction')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vice',\n",
       " 'President',\n",
       " 'Johnson',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Speaker',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Chief',\n",
       " 'Justice',\n",
       " ',',\n",
       " 'President',\n",
       " 'Eisenhower',\n",
       " ',',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'Nixon',\n",
       " ',',\n",
       " 'President',\n",
       " 'Truman',\n",
       " ',',\n",
       " 'reverend',\n",
       " 'clergy',\n",
       " ',',\n",
       " 'fellow',\n",
       " 'citizens',\n",
       " ',',\n",
       " 'we',\n",
       " 'observe',\n",
       " 'today',\n",
       " 'not',\n",
       " 'a',\n",
       " 'victory',\n",
       " 'of',\n",
       " 'party',\n",
       " ',',\n",
       " 'but',\n",
       " 'a',\n",
       " 'celebration',\n",
       " 'of',\n",
       " 'freedom',\n",
       " '--',\n",
       " 'symbolizing',\n",
       " 'an',\n",
       " 'end',\n",
       " ',',\n",
       " 'as',\n",
       " 'well']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.words(fileids='1961-Kennedy.txt')[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 4, 'of': 2, 'the': 2, 'and': 2, 'An': 1, 'essay': 1, 'is,': 1, 'generally,': 1, 'piece': 1, 'writing': 1, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=\"An essay is, generally, a piece of writing that gives the author's own argument — but the definition is vague, overlapping with those of a paper, an article, a pamphlet, and a short story. Essays have traditionally been sub-classified as formal and informal\"\n",
    "fd=nltk.FreqDist(text1.split())\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 13 conditions>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "cfd=ConditionalFreqDist((len(word),word) for word in text1.split())\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vice', 'President', 'Johnson', ',', 'Mr', '.', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.words(fileids='1961-Kennedy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'of': 2, 'An': 1, 'is': 1, 'an': 1, 'as': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=inaugural.words(fileids='1961-Kennedy.txt')\n",
    "cf=ConditionalFreqDist((len(word),word) for word in text1.split())\n",
    "cf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vice', 'President', 'Johnson', ',', 'Mr', '.', ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=inaugural.words(fileids='1961-Kennedy.txt')[0:10000]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1.tar.gz (19.2 MB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314481 sha256=da3230302b219aa7fd233883b3a146009ee458bf4550580451072f99c5174281\n",
      "  Stored in directory: c:\\users\\sanjay k\\appdata\\local\\pip\\cache\\wheels\\24\\aa\\17\\5bc7c72e9a37990a9620cc3aad0acad1564dcff6dbc2359de3\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\SANJAY~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 7.157 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg = jieba.cut(\"把句子中所有的可以成词的词语都扫描出来\",cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'typing', 'some', 'random', 'stuffs', 'to', 'check', 'the', 'program', \"'s\", 'working.this', 'is', 'an', 'example', '.']\n",
      "[\"Just typing some random stuffs to check the program's working.this is an example .\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,TweetTokenizer\n",
    "txt = \"Just typing some random stuffs to check the program's working.this is an example . \"\n",
    "print(word_tokenize(txt))\n",
    "print(sent_tokenize(txt))\n",
    "for t in txt:\n",
    "    s=sent_tokenize(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jist', 'typing', 'some', 'random', 'stuffs', 'to', 'check', 'the', 'program', \"'s\", 'working']\n",
      "[('Jist', 'NNP'), ('typing', 'VBG'), ('some', 'DT'), ('random', 'JJ'), ('stuffs', 'NNS'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('program', 'NN'), (\"'s\", 'POS'), ('working', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences :\n",
    "        word = nltk.word_tokenize(sentence)\n",
    "        print(word)\n",
    "        tagged = nltk.pos_tag(word)\n",
    "        print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'party', 'was', 'sooo', '&', '%', '^', 'fun', ':D', '#superfun']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='The party was sooo & % ^ fun :D #superfun'\n",
    "tt=TweetTokenizer()\n",
    "tt.tokenize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
